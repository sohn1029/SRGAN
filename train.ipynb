{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b0a83bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from math import log10\n",
    "\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.utils as utils\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pytorch_ssim\n",
    "from data_utils import TrainDatasetLoader, ValidationDatasetLoader, display_transform\n",
    "from loss import GeneratorLoss\n",
    "from model import Generator, Discriminator\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9781102",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_l = ['./epochs', './statistics', './data']\n",
    "for p in path_l:\n",
    "    if not os.path.exists(p):\n",
    "        os.makedirs(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef91c4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "UPSCALE_FACTOR = 4\n",
    "CROP_SIZE = 88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9405c084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "\n",
    "train_set = TrainDatasetLoader(Path('./data/train'), crop_size=CROP_SIZE, factor=UPSCALE_FACTOR)\n",
    "val_set = ValidationDatasetLoader(Path('./data/valid'), factor=UPSCALE_FACTOR)\n",
    "train_loader = DataLoader(dataset=train_set, num_workers=1, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_set, num_workers=1, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afb77845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model setting\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "netG = Generator(UPSCALE_FACTOR).cuda()\n",
    "netD = Discriminator().cuda()\n",
    "\n",
    "generator_criterion = GeneratorLoss().cuda()\n",
    "\n",
    "optimizerG = optim.Adam(netG.parameters())\n",
    "optimizerD = optim.Adam(netD.parameters())\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bcd9570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1/100] Loss_D: 0.9144 Loss_G: 0.0057 D(x): 0.1396 D(G(z)): 0.0456: 100%|████████████| 743/743 [05:47<00:00,  2.14it/s]\n",
      "[converting LR images to SR images] PSNR: 28.5551 dB SSIM: 0.8983: 100%|█████████████| 500/500 [00:31<00:00, 15.63it/s]\n",
      "[saving training results]: 100%|█████████████████████████████████████████████████████| 100/100 [01:06<00:00,  1.50it/s]\n",
      "[2/100] Loss_D: 1.0003 Loss_G: 0.0029 D(x): 0.0287 D(G(z)): 0.0280: 100%|████████████| 743/743 [05:48<00:00,  2.13it/s]\n",
      "[converting LR images to SR images] PSNR: 28.8533 dB SSIM: 0.9165: 100%|█████████████| 500/500 [00:32<00:00, 15.55it/s]\n",
      "[saving training results]: 100%|█████████████████████████████████████████████████████| 100/100 [01:07<00:00,  1.48it/s]\n",
      "[3/100] Loss_D: 1.0000 Loss_G: 0.0023 D(x): 0.0001 D(G(z)): 0.0001: 100%|████████████| 743/743 [05:51<00:00,  2.11it/s]\n",
      "[converting LR images to SR images] PSNR: 26.5644 dB SSIM: 0.9176: 100%|█████████████| 500/500 [00:32<00:00, 15.48it/s]\n",
      "[saving training results]: 100%|█████████████████████████████████████████████████████| 100/100 [01:08<00:00,  1.46it/s]\n",
      "[4/100] Loss_D: 1.0000 Loss_G: 0.0021 D(x): 0.0000 D(G(z)): 0.0000: 100%|████████████| 743/743 [05:55<00:00,  2.09it/s]\n",
      "[converting LR images to SR images] PSNR: 31.2482 dB SSIM: 0.9329: 100%|█████████████| 500/500 [00:31<00:00, 15.66it/s]\n",
      "[saving training results]: 100%|█████████████████████████████████████████████████████| 100/100 [01:06<00:00,  1.50it/s]\n",
      "[5/100] Loss_D: 1.0000 Loss_G: 0.0020 D(x): 0.0000 D(G(z)): 0.0000: 100%|████████████| 743/743 [05:56<00:00,  2.08it/s]\n",
      "[converting LR images to SR images] PSNR: 32.2026 dB SSIM: 0.9371: 100%|█████████████| 500/500 [00:31<00:00, 16.04it/s]\n",
      "[saving training results]: 100%|█████████████████████████████████████████████████████| 100/100 [01:05<00:00,  1.52it/s]\n",
      "[6/100] Loss_D: 1.0000 Loss_G: 0.0018 D(x): 0.0000 D(G(z)): 0.0000: 100%|████████████| 743/743 [05:58<00:00,  2.07it/s]\n",
      "[converting LR images to SR images] PSNR: 30.5219 dB SSIM: 0.9336: 100%|█████████████| 500/500 [00:31<00:00, 16.05it/s]\n",
      "[saving training results]: 100%|█████████████████████████████████████████████████████| 100/100 [01:06<00:00,  1.50it/s]\n",
      "[7/100] Loss_D: 1.0000 Loss_G: 0.0018 D(x): 0.0000 D(G(z)): 0.0000: 100%|████████████| 743/743 [05:54<00:00,  2.09it/s]\n",
      "[converting LR images to SR images] PSNR: 27.9032 dB SSIM: 0.9353: 100%|█████████████| 500/500 [00:31<00:00, 15.90it/s]\n",
      "[saving training results]: 100%|█████████████████████████████████████████████████████| 100/100 [01:07<00:00,  1.49it/s]\n",
      "[8/100] Loss_D: 1.0000 Loss_G: 0.0020 D(x): 0.0000 D(G(z)): 0.0000:   4%|▍            | 27/743 [00:14<06:32,  1.82it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\GEONHO~1\\AppData\\Local\\Temp/ipykernel_46504/752944845.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mg_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator_criterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfake_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfake_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mg_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mfake_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = {'d_loss':[], 'g_loss':[], 'd_score':[], 'g_score':[], 'psnr':[], 'ssim':[]}\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    train_bar = tqdm(train_loader)\n",
    "    running_results = {'batch_sizes':0, 'd_loss':0, 'g_loss':0, 'd_score':0, 'g_score':0}\n",
    "    \n",
    "    netG.train()\n",
    "    netD.train()\n",
    "    for data, target in train_bar:\n",
    "        g_update_first = True\n",
    "        batch_size = data.size(0)\n",
    "        running_results['batch_sizes'] += batch_size\n",
    "        \n",
    "        # Train Descriminator network : maximize D(x) - 1 - D(G(z))\n",
    "        # Give a big score for choosing the original image\n",
    "        real_img = Variable(target).cuda()\n",
    "        z = Variable(data).cuda()\n",
    "        \n",
    "        fake_img = netG(z)\n",
    "        netD.zero_grad()\n",
    "        \n",
    "        real_out = netD(real_img).mean()\n",
    "        fake_out = netD(fake_img).mean()\n",
    "        \n",
    "        d_loss = 1 - real_out + fake_out\n",
    "        \n",
    "        d_loss.backward(retain_graph = True)\n",
    "        optimizerD.step()\n",
    "        \n",
    "        # train Generator network : minimize 1 - D(G(z)) + Perception Loss + Image Loss + TV Loss\n",
    "        #\n",
    "        netG.zero_grad()\n",
    "        \n",
    "        fake_img = netG(z)\n",
    "        fake_out = netD(fake_img).mean()\n",
    "        \n",
    "        g_loss = generator_criterion(fake_out, fake_img, real_img)\n",
    "        g_loss.backward()\n",
    "        \n",
    "        fake_img = netG(z)\n",
    "        fake_out = netD(fake_img).mean()\n",
    "        \n",
    "        optimizerG.step()\n",
    "        \n",
    "        running_results['g_loss'] += g_loss.item() * batch_size\n",
    "        running_results['d_loss'] += d_loss.item() * batch_size\n",
    "        running_results['d_score'] += real_out.item() * batch_size\n",
    "        running_results['g_score'] += fake_out.item() * batch_size\n",
    "        \n",
    "        train_bar.set_description(desc = '[%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f' % (\n",
    "            epoch, NUM_EPOCHS, running_results['d_loss'] / running_results['batch_sizes'],\n",
    "            running_results['g_loss'] / running_results['batch_sizes'],\n",
    "            running_results['d_score'] / running_results['batch_sizes'],\n",
    "            running_results['g_score'] / running_results['batch_sizes'],\n",
    "        ))\n",
    "        \n",
    "    netG.eval()\n",
    "    out_path = 'training_results/SRF_' + str(UPSCALE_FACTOR) + '/'\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        val_bar = tqdm(val_loader)\n",
    "        valing_results = {'mse': 0, 'ssims': 0, 'psnr': 0, 'ssim': 0, 'batch_sizes': 0}\n",
    "        val_images = []\n",
    "        for val_lr, val_hr_restore, val_hr in val_bar:\n",
    "            batch_size = val_lr.size(0)\n",
    "            valing_results['batch_sizes'] += batch_size\n",
    "            lr = val_lr.cuda()\n",
    "            hr = val_hr.cuda()\n",
    "            sr = netG(lr)\n",
    "            \n",
    "            batch_mse = ((sr-hr)**2).data.mean()\n",
    "            valing_results['mse'] += batch_mse * batch_size\n",
    "            \n",
    "            batch_ssim = pytorch_ssim.ssim(sr, hr).item()\n",
    "            \n",
    "            valing_results['ssims'] += batch_ssim * batch_size\n",
    "            \n",
    "            # PSNR (Peak Signal-to-Noise Ratio)\n",
    "            # PSNR = 10 * log10( R^2 / MSE )\n",
    "            \n",
    "            valing_results['psnr'] = 10 * log10((1**2) / (valing_results['mse'] / valing_results['batch_sizes']))\n",
    "            #valing_results['psnr'] = 10 * log10((hr.max()**2) / (valing_results['mse'] / valing_results['batch_sizes']))\n",
    "            \n",
    "            valing_results['ssim'] = valing_results['ssims'] / valing_results['batch_sizes']\n",
    "            val_bar.set_description(\n",
    "                desc = '[converting LR images to SR images] PSNR: %.4f dB SSIM: %.4f' % (\n",
    "                valing_results['psnr'], valing_results['ssim']))\n",
    "            \n",
    "            val_images.extend(\n",
    "                [display_transform()(val_hr_restore.squeeze(0)), display_transform()(hr.data.cpu().squeeze(0)),\n",
    "                display_transform()(sr.data.cpu().squeeze(0))])\n",
    "            \n",
    "        val_images = torch.stack(val_images)\n",
    "        val_images = torch.chunk(val_images, val_images.size(0) // 15)\n",
    "        val_save_bar = tqdm(val_images, desc = '[saving training results]')\n",
    "        index = 1\n",
    "        for image in val_save_bar:\n",
    "            image = utils.make_grid(image, nrow = 3, padding = 5)\n",
    "            utils.save_image(image, out_path + 'epoch_%d_index_%d.png' % (epoch, index), padding = 5)\n",
    "            index += 1\n",
    "        \n",
    "        # save model\n",
    "        torch.save(netG.state_dict(), 'epochs/netG_epoch_%d_%d.pth' % (UPSCALE_FACTOR, epoch))\n",
    "        torch.save(netD.state_dict(), 'epochs/netD_epoch_%d_%d.pth' % (UPSCALE_FACTOR, epoch))\n",
    "        \n",
    "        results['d_loss'].append(running_results['d_loss'] / running_results['batch_sizes'])\n",
    "        results['g_loss'].append(running_results['g_loss'] / running_results['batch_sizes'])\n",
    "        results['d_score'].append(running_results['d_score'] / running_results['batch_sizes'])\n",
    "        results['g_score'].append(running_results['g_score'] / running_results['batch_sizes'])\n",
    "        results['psnr'].append(valing_results['psnr'])\n",
    "        results['ssim'].append(valing_results['ssim'])\n",
    "        \n",
    "        \n",
    "        if epoch%10 == 0 and epoch != 0:\n",
    "            out_path = 'statistics/'\n",
    "            data_frame = pd.DataFrame(\n",
    "                data = {'Loss_D': results['d_loss'], 'Loss_G': results['g_loss'], 'Score_D': results['d_score'],\n",
    "                       'Score_G': results['g_score'], 'PSNR': results['psnr'], 'SSIM': results['ssim']},\n",
    "                       index = range(1, epoch + 1))\n",
    "            data_frame.to_csv(out_path + 'srf_' + str(UPSCALE_FACTOR) + '_train_results.csv', index_label = 'Epoch')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1a45c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
