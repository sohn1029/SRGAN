{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b0a83bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from math import log10\n",
    "\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.utils as utils\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pytorch_ssim\n",
    "from data_utils import TrainDatasetLoader, ValidationDatasetLoader, display_transform\n",
    "from loss import GeneratorLoss\n",
    "from model import Generator, Discriminator\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9781102",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_l = ['./epochs', './statistics', './data']\n",
    "for p in path_l:\n",
    "    if not os.path.exists(p):\n",
    "        os.makedirs(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef91c4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "UPSCALE_FACTOR = 4\n",
    "CROP_SIZE = 88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9405c084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "\n",
    "train_set = TrainDatasetLoader(Path('./data/train'), crop_size=CROP_SIZE, factor=UPSCALE_FACTOR)\n",
    "val_set = ValidationDatasetLoader(Path('./data/valid'), factor=UPSCALE_FACTOR)\n",
    "train_loader = DataLoader(dataset=train_set, num_workers=1, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_set, num_workers=1, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afb77845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model setting\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "netG = Generator(UPSCALE_FACTOR).cuda()\n",
    "netD = Discriminator().cuda()\n",
    "\n",
    "generator_criterion = GeneratorLoss().cuda()\n",
    "\n",
    "optimizerG = optim.Adam(netG.parameters())\n",
    "optimizerD = optim.Adam(netD.parameters())\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcd9570",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'d_loss':[], 'g_loss':[], 'd_score':[], 'g_score':[], 'psnr':[], 'ssim':[]}\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    train_bar = tqdm(train_loader)\n",
    "    running_results = {'batch_sizes':0, 'd_loss':0, 'g_loss':0, 'd_score':0, 'g_score':0}\n",
    "    \n",
    "    netG.train()\n",
    "    netD.train()\n",
    "    for data, target in train_bar:\n",
    "        g_update_first = True\n",
    "        batch_size = data.size(0)\n",
    "        running_results['batch_sizes'] += batch_size\n",
    "        \n",
    "        # Train Descriminator network : maximize D(x) - 1 - D(G(z))\n",
    "        # Give a big score for choosing the original image\n",
    "        real_img = Variable(target).cuda()\n",
    "        z = Variable(data).cuda()\n",
    "        \n",
    "        fake_img = netG(z)\n",
    "        netD.zero_grad()\n",
    "        \n",
    "        real_out = netD(real_img).mean()\n",
    "        fake_out = netD(fake_img).mean()\n",
    "        \n",
    "        d_loss = 1 - real_out + fake_out\n",
    "        \n",
    "        d_loss.backward(retain_graph = True)\n",
    "        optimizerD.step()\n",
    "        \n",
    "        # train Generator network : minimize 1 - D(G(z)) + Perception Loss + Image Loss + TV Loss\n",
    "        #\n",
    "        netG.zero_grad()\n",
    "        \n",
    "        fake_img = netG(z)\n",
    "        fake_out = netD(fake_img).mean()\n",
    "        \n",
    "        g_loss = generator_criterion(fake_out, fake_img, real_img)\n",
    "        g_loss.backward()\n",
    "        \n",
    "        fake_img = netG(z)\n",
    "        fake_out = netD(fake_img).mean()\n",
    "        \n",
    "        optimizerG.step()\n",
    "        \n",
    "        running_results['g_loss'] += g_loss.item() * batch_size\n",
    "        running_results['d_loss'] += d_loss.item() * batch_size\n",
    "        running_results['d_score'] += real_out.item() * batch_size\n",
    "        running_results['g_score'] += fake_out.item() * batch_size\n",
    "        \n",
    "        train_bar.set_description(desc = '[%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f' % (\n",
    "            epoch, NUM_EPOCHS, running_results['d_loss'] / running_results['batch_sizes'],\n",
    "            running_results['g_loss'] / running_results['batch_sizes'],\n",
    "            running_results['d_score'] / running_results['batch_sizes'],\n",
    "            running_results['g_score'] / running_results['batch_sizes'],\n",
    "        ))\n",
    "        \n",
    "    netG.eval()\n",
    "    out_path = 'training_results/SRF_' + str(UPSCALE_FACTOR) + '/'\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        val_bar = tqdm(val_loader)\n",
    "        valing_results = {'mse': 0, 'ssims': 0, 'psnr': 0, 'ssim': 0, 'batch_sizes': 0}\n",
    "        val_images = []\n",
    "        for val_lr, val_hr_restore, val_hr in val_bar:\n",
    "            batch_size = val_lr.size(0)\n",
    "            valing_results['batch_sizes'] += batch_size\n",
    "            lr = val_lr.cuda()\n",
    "            hr = val_hr.cuda()\n",
    "            sr = netG(lr)\n",
    "            \n",
    "            batch_mse = ((sr-hr)**2).data.mean()\n",
    "            valing_results['mse'] += batch_mse * batch_size\n",
    "            \n",
    "            batch_ssim = pytorch_ssim.ssim(sr, hr).item()\n",
    "            \n",
    "            valing_results['ssims'] += batch_ssim * batch_size\n",
    "            \n",
    "            # PSNR (Peak Signal-to-Noise Ratio)\n",
    "            # PSNR = 10 * log10( R^2 / MSE )\n",
    "            \n",
    "            valing_results['psnr'] = 10 * log10((1**2) / (valing_results['mse'] / valing_results['batch_sizes']))\n",
    "            #valing_results['psnr'] = 10 * log10((hr.max()**2) / (valing_results['mse'] / valing_results['batch_sizes']))\n",
    "            \n",
    "            valing_results['ssim'] = valing_results['ssims'] / valing_results['batch_sizes']\n",
    "            val_bar.set_description(\n",
    "                desc = '[converting LR images to SR images] PSNR: %.4f dB SSIM: %.4f' % (\n",
    "                valing_results['psnr'], valing_results['ssim']))\n",
    "            \n",
    "            val_images.extend(\n",
    "                [display_transform()(val_hr_restore.squeeze(0)), display_transform()(hr.data.cpu().squeeze(0)),\n",
    "                display_transform()(sr.data.cpu().squeeze(0))])\n",
    "            \n",
    "        val_images = torch.stack(val_images)\n",
    "        val_images = torch.chunk(val_images, val_images.size(0) // 15)\n",
    "        val_save_bar = tqdm(val_images, desc = '[saving training results]')\n",
    "        index = 1\n",
    "        for image in val_save_bar:\n",
    "            image = utils.make_grid(image, nrow = 3, padding = 5)\n",
    "            utils.save_image(image, out_path + 'epoch_%d_index_%d.png' % (epoch, index), padding = 5)\n",
    "            index += 1\n",
    "        \n",
    "        # save model\n",
    "        torch.save(netG.state_dict(), 'epochs/netG_epoch_%d_%d.pth' % (UPSCALE_FACTOR, epoch))\n",
    "        torch.save(netD.state_dict(), 'epochs/netD_epoch_%d_%d.pth' % (UPSCALE_FACTOR, epoch))\n",
    "        \n",
    "        results['d_loss'].append(running_results['d_loss'] / running_results['batch_sizes'])\n",
    "        results['g_loss'].append(running_results['g_loss'] / running_results['batch_sizes'])\n",
    "        results['d_score'].append(running_results['d_score'] / running_results['batch_sizes'])\n",
    "        results['g_score'].append(running_results['g_score'] / running_results['batch_sizes'])\n",
    "        results['psnr'].append(valing_results['psnr'])\n",
    "        results['ssim'].append(valing_results['ssim'])\n",
    "        \n",
    "        \n",
    "        if epoch%10 == 0 and epoch != 0:\n",
    "            out_path = 'statistics/'\n",
    "            data_frame = pd.DataFrame(\n",
    "                data = {'Loss_D': results['d_loss'], 'Loss_G': results['g_loss'], 'Score_D': results['d_score'],\n",
    "                       'Score_G': results['g_score'], 'PSNR': results['psnr'], 'SSIM': results['ssim']},\n",
    "                       index = range(1, epoch + 1))\n",
    "            data_frame.to_csv(out_path + 'srf_' + str(UPSCALE_FACTOR) + '_train_results.csv', index_label = 'Epoch')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1a45c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
